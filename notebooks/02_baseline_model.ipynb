{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *\n",
    "import joblib\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "print(\"Ready for baseline model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/preprocessed_creditcard.csv')\n",
    "train_indices = np.load('../data/train_indices.npy')\n",
    "test_indices = np.load('../data/test_indices.npy')\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "X_train = X.iloc[train_indices]\n",
    "y_train = y.iloc[train_indices]\n",
    "X_test = X.iloc[test_indices]\n",
    "y_test = y.iloc[test_indices]\n",
    "print(f\"Train: {len(X_train):,} | Test: {len(X_test):,} | Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3: Train Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = LogisticRegression(random_state=42, max_iter=100, solver='liblinear', class_weight='balanced')\n",
    "start = time.time()\n",
    "baseline_model.fit(X_train, y_train)\n",
    "training_time = time.time() - start\n",
    "y_pred = baseline_model.predict(X_test)\n",
    "y_pred_proba = baseline_model.predict_proba(X_test)[:, 1]\n",
    "print(f\"Trained in {training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4: Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "cost_per_fp, cost_per_fn = 10, 100\n",
    "total_cost = fp * cost_per_fp + fn * cost_per_fn\n",
    "print(f\"FP: {fp:,} | FN: {fn} | Cost: ${total_cost:,} | AUC: {roc_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 5: Business Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Confusion Matrix\n",
    "ax1 = plt.subplot(2,3,1)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Legit','Fraud'], yticklabels=['Legit','Fraud'])\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Business Impact\n",
    "ax2 = plt.subplot(2,3,2)\n",
    "plt.bar(['FP\\n(Angry)', 'FN\\n(Lost)', 'TP\\n(Caught)', 'TN\\n(Happy)'], [fp, fn, tp, tn], \n",
    "        color=['orange','red','green','lightgreen'])\n",
    "plt.title(f'Business Impact (Cost: ${total_cost:,})')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# ROC Curve\n",
    "ax3 = plt.subplot(2,3,3)\n",
    "plt.plot(fpr, tpr, 'b-', label=f'AUC={roc_auc:.3f}', linewidth=2)\n",
    "plt.plot([0,1], [0,1], 'r--', alpha=0.5)\n",
    "thresh_idx = np.argmin(np.abs(thresholds - 0.5))\n",
    "plt.plot(fpr[thresh_idx], tpr[thresh_idx], 'go', markersize=10)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Threshold Analysis\n",
    "ax4 = plt.subplot(2,3,4)\n",
    "test_thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "fps_list = []\n",
    "fns_list = []\n",
    "for t in test_thresholds:\n",
    "    pred_t = (y_pred_proba >= t).astype(int)\n",
    "    cm_t = confusion_matrix(y_test, pred_t)\n",
    "    fps_list.append(cm_t[0,1])\n",
    "    fns_list.append(cm_t[1,0])\n",
    "plt.plot(test_thresholds, fps_list, 'o-', label='FP', color='orange')\n",
    "plt.plot(test_thresholds, fns_list, 's-', label='FN', color='red')\n",
    "plt.axvline(0.5, color='green', linestyle='--')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Errors')\n",
    "plt.title('Threshold Impact')\n",
    "plt.legend()\n",
    "\n",
    "# Summary\n",
    "ax5 = plt.subplot(2,3,(5,6))\n",
    "ax5.axis('off')\n",
    "summary = f\"\"\"\n",
    "BASELINE MODEL PERFORMANCE\n",
    "{'='*40}\n",
    "Accuracy: {accuracy:.2%} | Precision: {precision:.2%}\n",
    "Recall: {recall:.2%} | F1: {f1:.2%}\n",
    "ROC-AUC: {roc_auc:.3f}\n",
    "\n",
    "BUSINESS IMPACT\n",
    "False Positives: {fp:,} (${fp*cost_per_fp:,})\n",
    "False Negatives: {fn} (${fn*cost_per_fn:,})\n",
    "Total Cost: ${total_cost:,}\n",
    "Train Time: {training_time:.2f}s\n",
    "\"\"\"\n",
    "ax5.text(0.5, 0.5, summary, transform=ax5.transAxes, fontsize=11, ha='center', va='center',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8), family='monospace')\n",
    "\n",
    "plt.suptitle('Baseline Model Dashboard', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 6: Save Model & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../outputs', exist_ok=True)\n",
    "joblib.dump(baseline_model, '../models/baseline_logistic_regression.pkl')\n",
    "joblib.dump(baseline_model, '../models/baseline_for_comparison.pkl')\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': float(accuracy), 'precision': float(precision), 'recall': float(recall),\n",
    "    'f1_score': float(f1), 'roc_auc': float(roc_auc), 'false_positives': int(fp),\n",
    "    'false_negatives': int(fn), 'true_positives': int(tp), 'true_negatives': int(tn),\n",
    "    'total_cost': float(total_cost), 'training_time': float(training_time)\n",
    "}\n",
    "with open('../outputs/baseline_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\"\"\n",
    "{'='*60}\n",
    "BASELINE COMPLETE!\n",
    "FP: {fp:,} | FN: {fn} | Cost: ${total_cost:,} | AUC: {roc_auc:.3f}\n",
    "Challenge: Reduce FP by 30% (target: <{int(fp*0.7):,})\n",
    "{'='*60}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}